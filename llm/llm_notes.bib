@electronic{wolfram_2023, url={https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/}, author={{Stephen Wolfram Writings}}, year={2023}, month={Feb}, lastchecked = {2023-03-01} }

@article{carolin_2022,
  doi = {10.34732/XDVBLG-QSBTYX},
  url = {https://x-dev.pages.jsc.fz-juelich.de/2022/07/13/transformers-matmul.html},
  author = {Penke,  Carolin},
  keywords = {Workshop,  OpenGPTX},
  language = {en},
  title = {A mathematician's introduction to transformers and large language models},
  publisher = {Accelerating Devices Lab},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{attention_2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@electronic{annotated_trans, url = {http://nlp.seas.harvard.edu/annotated-transformer/}, author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia}, title = {The Annotated Transformer}}

@electronic{colah, url={https://colah.github.io/posts/2015-08-Understanding-LSTMs/}, author={Christopher Olah}, year={2015}, month={Aug} }

@unpublished{spacy2, 
    AUTHOR = {Honnibal, Matthew and Montani, Ines}, 
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing}, 
    YEAR   = {2017}, 
    Note   = {To appear} 
}

@incollection{pytorch, 
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, 
booktitle = {Advances in Neural Information Processing Systems 32}, 
pages = {8024--8035}, 
year = {2019}, 
publisher = {Curran Associates, Inc.}, 
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} 
}

@article{llama2023,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}